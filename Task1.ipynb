{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import NotFound\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean full data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define data types for reading in\n",
    "\n",
    "dtypes = {\n",
    "    0: 'str',    # datetime (index 0) - TIMESTAMP mapped to string for initial import\n",
    "    1: 'float',  # register_no (index 1) - FLOAT\n",
    "    2: 'float',  # emp_no (index 2) - FLOAT\n",
    "    3: 'float',  # trans_no (index 3) - FLOAT\n",
    "    4: 'str',    # upc (index 4) - STRING\n",
    "    5: 'str',    # description (index 5) - STRING\n",
    "    6: 'str',    # trans_type (index 6) - STRING\n",
    "    7: 'str',    # trans_subtype (index 7) - STRING\n",
    "    8: 'str',    # trans_status (index 8) - STRING\n",
    "    9: 'float',  # department (index 9) - FLOAT\n",
    "    10: 'float', # quantity (index 10) - FLOAT\n",
    "    11: 'float', # Scale (index 11) - FLOAT\n",
    "    12: 'float', # cost (index 12) - FLOAT\n",
    "    13: 'float', # unitPrice (index 13) - FLOAT\n",
    "    14: 'float', # total (index 14) - FLOAT\n",
    "    15: 'float', # regPrice (index 15) - FLOAT\n",
    "    16: 'float', # altPrice (index 16) - FLOAT\n",
    "    17: 'float', # tax (index 17) - FLOAT\n",
    "    18: 'float', # taxexempt (index 18) - FLOAT\n",
    "    19: 'float', # foodstamp (index 19) - FLOAT\n",
    "    20: 'float', # wicable (index 20) - FLOAT\n",
    "    21: 'float', # discount (index 21) - FLOAT\n",
    "    22: 'float', # memDiscount (index 22) - FLOAT\n",
    "    23: 'float', # discountable (index 23) - FLOAT\n",
    "    24: 'float', # discounttype (index 24) - FLOAT\n",
    "    25: 'float', # voided (index 25) - FLOAT\n",
    "    26: 'float', # percentDiscount (index 26) - FLOAT\n",
    "    27: 'float', # ItemQtty (index 27) - FLOAT\n",
    "    28: 'float', # volDiscType (index 28) - FLOAT\n",
    "    29: 'float', # volume (index 29) - FLOAT\n",
    "    30: 'float', # VolSpecial (index 30) - FLOAT\n",
    "    31: 'float', # mixMatch (index 31) - FLOAT\n",
    "    32: 'float', # matched (index 32) - FLOAT\n",
    "    33: 'str',   # memType (index 33) - STRING\n",
    "    34: 'str',   # staff (index 34) - STRING\n",
    "    35: 'float', # numflag (index 35) - FLOAT\n",
    "    36: 'float', # itemstatus (index 36) - FLOAT\n",
    "    37: 'float', # tenderstatus (index 37) - FLOAT\n",
    "    38: 'str',   # charflag (index 38) - STRING\n",
    "    39: 'float', # varflag (index 39) - FLOAT\n",
    "    40: 'str',   # batchHeaderID (index 40) - STRING\n",
    "    41: 'float', # local (index 41) - FLOAT\n",
    "    42: 'str',   # organic (index 42) - STRING (changed from FLOAT to STRING)\n",
    "    43: 'str',   # display (index 43) - STRING\n",
    "    44: 'float', # receipt (index 44) - FLOAT\n",
    "    45: 'float', # card_no (index 45) - FLOAT\n",
    "    46: 'float', # store (index 46) - FLOAT\n",
    "    47: 'float', # branch (index 47) - FLOAT\n",
    "    48: 'float', # match_id (index 48) - FLOAT\n",
    "    49: 'float'  # trans_id (index 49) - FLOAT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_columns = [\n",
    "    'datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description', \n",
    "    'trans_type', 'trans_subtype', 'trans_status', 'department', 'quantity', \n",
    "    'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', \n",
    "    'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', \n",
    "    'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty', \n",
    "    'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType', \n",
    "    'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag', 'varflag', \n",
    "    'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no', \n",
    "    'store', 'branch', 'match_id', 'trans_id'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfolder_path = \\'data/Wedge_full/\\'\\n\\n# Function to read a CSV file and detect the delimiter\\ndef read_csv_with_delimiter(file_path, expected_columns=expected_columns, dtypes=dtypes):\\n    try:\\n        # Try reading with a comma as the delimiter, with dtypes applied\\n        df = pd.read_csv(file_path, dtype=dtypes)\\n        \\n        # Check if the number of columns matches the expected number\\n        if df.shape[1] != expected_columns:\\n            print(f\"Column mismatch with comma delimiter, trying with semicolon...\")\\n            # Try reading with a semicolon as the delimiter\\n            df = pd.read_csv(file_path, delimiter=\\';\\', dtype=dtypes)\\n        \\n        return df\\n    except Exception as e:\\n        print(f\\'Error reading {file_path}: {e}\\')\\n        return None\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not using currently, use cell below for data with \\\\N\n",
    "'''\n",
    "\n",
    "folder_path = 'data/Wedge_full/'\n",
    "\n",
    "# Function to read a CSV file and detect the delimiter\n",
    "def read_csv_with_delimiter(file_path, expected_columns=expected_columns, dtypes=dtypes):\n",
    "    try:\n",
    "        # Try reading with a comma as the delimiter, with dtypes applied\n",
    "        df = pd.read_csv(file_path, dtype=dtypes)\n",
    "        \n",
    "        # Check if the number of columns matches the expected number\n",
    "        if df.shape[1] != expected_columns:\n",
    "            print(f\"Column mismatch with comma delimiter, trying with semicolon...\")\n",
    "            # Try reading with a semicolon as the delimiter\n",
    "            df = pd.read_csv(file_path, delimiter=';', dtype=dtypes)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {file_path}: {e}')\n",
    "        return None\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Alternate for above when \\\\N is present\n",
    "folder_path = 'data/Wedge_full/'\n",
    "\n",
    "def read_csv_with_delimiter(file_path, expected_columns=expected_columns, dtypes=dtypes):\n",
    "    try:\n",
    "        # Step 1: Try reading with a comma as the delimiter, treating '\\\\N' as NaN\n",
    "        df = pd.read_csv(file_path, dtype=dtypes, na_values='\\\\N')\n",
    "\n",
    "        # Step 2: Check if the number of columns matches the expected number\n",
    "        if df.shape[1] != len(expected_columns):\n",
    "            print(f\"Column mismatch with comma delimiter in {file_path}, trying with semicolon...\")\n",
    "\n",
    "            # Try reading with a semicolon as the delimiter, treating '\\\\N' as NaN\n",
    "            df = pd.read_csv(file_path, delimiter=';', dtype=dtypes, na_values='\\\\N')\n",
    "\n",
    "        # Step 3: If the first row looks like headers (columns don't match), treat it as data\n",
    "        if list(df.columns) != expected_columns:\n",
    "            print(f\"Header mismatch detected in {file_path}. Moving first row to data and applying expected columns.\")\n",
    "            \n",
    "            # Move current headers into the first row and reset the index\n",
    "            df.columns = range(df.shape[1])  # Temporarily rename columns with numeric indices\n",
    "            df = pd.concat([pd.DataFrame([df.columns], columns=df.columns), df], ignore_index=True)\n",
    "\n",
    "            # Assign expected_columns as headers\n",
    "            df.columns = expected_columns\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {file_path}: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "slice_length = 5     # 3 took 15 minutes\n",
    "slices = [csv_files[i:i+slice_length] for i in range(0, len(csv_files), slice_length)]\n",
    "\n",
    "slice_to_process = 0 # change this to the slice you want to process\n",
    "csv_files = slices[slice_to_process]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transArchive_201304_201306_inactive.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column mismatch with comma delimiter in data/Wedge_full/transArchive_201304_201306_inactive.csv, trying with semicolon...\n"
     ]
    }
   ],
   "source": [
    "# Read in the CSV files using the defined function\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    # Read in the data\n",
    "    df = read_csv_with_delimiter(file_path, expected_columns, dtypes=dtypes)\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    if df is not None:\n",
    "        dataframes.append(df)\n",
    "    else:\n",
    "        print(f'Error reading dataframe for file {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame 1:\n",
      "              datetime  register_no  emp_no  trans_no            upc  \\\n",
      "0  2013-04-01 09:28:38          2.0    44.0       2.0  0076333201701   \n",
      "1  2013-04-01 09:28:42          2.0    44.0       2.0  0089348100141   \n",
      "2  2013-04-01 09:28:42          2.0    44.0       2.0  0089348100141   \n",
      "3  2013-04-01 09:28:45          2.0    44.0       2.0  0066529000303   \n",
      "4  2013-04-01 09:29:01          2.0    44.0       2.0              0   \n",
      "\n",
      "                      description trans_type trans_subtype trans_status  \\\n",
      "0  Sport-Top Steel Bottle 12oz KK          I                              \n",
      "1    Hand Sanitizer Wipes 10ct CW          I                              \n",
      "2    Hand Sanitizer Wipes 10ct CW          I                              \n",
      "3           Blackberries 6oz pkg.          I                              \n",
      "4                     Credit Card          T            CC                \n",
      "\n",
      "   department  ...  batchHeaderID  local  organic  display  receipt  card_no  \\\n",
      "0         9.0  ...            NaN    0.0        0               0.0  47885.0   \n",
      "1        11.0  ...            NaN    0.0        0               0.0  47885.0   \n",
      "2        11.0  ...            NaN    0.0        0               0.0  47885.0   \n",
      "3         2.0  ...            NaN    0.0        0               0.0  47885.0   \n",
      "4         0.0  ...            NaN    0.0      NaN               0.0  47885.0   \n",
      "\n",
      "   store  branch  match_id  trans_id  \n",
      "0    1.0     0.0       0.0       1.0  \n",
      "1    1.0     0.0       0.0       3.0  \n",
      "2    1.0     0.0       0.0       2.0  \n",
      "3    1.0     0.0       0.0       4.0  \n",
      "4    1.0     0.0       0.0      12.0  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"\\nDataFrame {i+1}:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 1:\n",
      "  Number of columns: 50\n",
      "  Columns match expected columns: True\n",
      "  Total null values: 633803\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Show number of columns, if columns match expected columns, and total null values\n",
    "def check_dataframe(df, df_index):\n",
    "    # Number of columns\n",
    "    num_columns = df.shape[1]\n",
    "    \n",
    "    # Check if column names match expected_columns\n",
    "    columns_match = set(df.columns) == set(expected_columns)\n",
    "    \n",
    "    # Count of null values in the dataset\n",
    "    null_count = df.isnull().sum().sum()\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"DataFrame {df_index + 1}:\")\n",
    "    print(f\"  Number of columns: {num_columns}\")\n",
    "    print(f\"  Columns match expected columns: {columns_match}\")\n",
    "    print(f\"  Total null values: {null_count}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Iterate through each DataFrame and check its properties\n",
    "for i, df in enumerate(dataframes):\n",
    "    check_dataframe(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rache\\AppData\\Local\\Temp\\ipykernel_26400\\593053506.py:26: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(r'^\\s*$', None, regex=True, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def clean_dataframe(df, expected_columns):\n",
    "    try:\n",
    "        # Remove \"\" from column headers\n",
    "        df.columns = df.columns.str.replace('\"', '', regex=False)\n",
    "\n",
    "        # Replace any \"\\N\" values in the headers with None (which will be NULL in SQL)\n",
    "        df.columns = [col if col != '\\\\N' else None for col in df.columns]\n",
    "        \n",
    "        # Replace problematic values with None (equivalent to NULL)\n",
    "        df.replace(['\\\\N', 'NULL', 'null', 'NaN', 'nan'], None, inplace=True)\n",
    "\n",
    "        # Remove \"\" from the data\n",
    "        df = df.replace('\"', '', regex=True)\n",
    "        df = df.replace(';', ',', regex=True)\n",
    "        \n",
    "        # Check if column headers match expected_columns\n",
    "        if list(df.columns) != expected_columns:\n",
    "            print(f\"Columns do not match, replacing with expected columns\")\n",
    "            df.columns = expected_columns\n",
    "\n",
    "            # Move current column headers (which are actual data) into the first row\n",
    "            df.columns = range(df.shape[1])  # Temporarily rename columns with numeric indices\n",
    "            df = pd.concat([pd.DataFrame([df.columns], columns=df.columns), df], ignore_index=True)\n",
    "\n",
    "        # Replace empty cells and any problematic values (e.g., \\N) with None (NULL)\n",
    "        df.replace(r'^\\s*$', None, regex=True, inplace=True)\n",
    "        df.replace('\\\\N', None, inplace=True)\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning dataframe: {e}\")\n",
    "        return None\n",
    "\n",
    "# Clean each dataframe in the list\n",
    "cleaned_dataframes = []\n",
    "for df in dataframes:\n",
    "    cleaned_df = clean_dataframe(df, expected_columns)\n",
    "    if cleaned_df is not None:\n",
    "        cleaned_dataframes.append(cleaned_df)\n",
    "    else:\n",
    "        print(f\"Error cleaning dataframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe at index 0 has more than one column, no action needed.\n"
     ]
    }
   ],
   "source": [
    "#for dataframes with only 1 column, resplit\n",
    "for i, df in enumerate(dataframes):  # Use enumerate to get both index and DataFrame\n",
    "    if df.shape[1] == 1:  # Check if the dataframe has only one column\n",
    "        print(f\"Dataframe at index {i} has only one column, attempting to split...\")\n",
    "\n",
    "        #Remove headers if there are any\n",
    "        if df.iloc[0, 0].startswith('datetime'):\n",
    "            df = df.iloc[1:]\n",
    "\n",
    "        # Step 1: Split the single column (data) into multiple columns based on commas\n",
    "        df_split = df.iloc[:, 0].str.split(',', expand=True)\n",
    "\n",
    "        # Step 2: Check for rows that have more than 50 columns\n",
    "        for idx, row in df_split.iterrows():\n",
    "            if len(row) > 50:\n",
    "                print(f\"Row {idx} in DataFrame {i} has {len(row)} columns:\\n{row}\")\n",
    "\n",
    "        # Step 3: Ensure the split column count matches the expected columns\n",
    "        if len(expected_columns) != df_split.shape[1]:\n",
    "            # Raise an error if there is a mismatch\n",
    "            raise ValueError(\n",
    "                f\"Column mismatch at DataFrame index {i}: Expected {len(expected_columns)} columns, \"\n",
    "                f\"but found {df_split.shape[1]} after splitting.\"\n",
    "            )\n",
    "\n",
    "        # Step 4: Assign the expected_columns as the new header\n",
    "        df_split.columns = expected_columns\n",
    "\n",
    "        # Replace the original DataFrame in the list with the new split DataFrame\n",
    "        dataframes[i] = df_split\n",
    "    else:\n",
    "        # If the DataFrame has more than one column, leave it as is\n",
    "        print(f\"Dataframe at index {i} has more than one column, no action needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change datatypes\n",
    "\n",
    "for df in dataframes:\n",
    "    for col, dtype in dtypes.items():\n",
    "        col_name = expected_columns[col]\n",
    "        try:\n",
    "            df[col_name] = df[col_name].astype(dtype)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error converting column '{col_name}' to {dtype}: {e}\")\n",
    "            df[col_name] = df[col_name].astype(str)\n",
    "            for idx, value in df[col_name].items():\n",
    "                if pd.isna(value):\n",
    "                    print(f\"Conversion failed at row {idx}, column '{col_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_directory = \"data/full_clean/\"\n",
    "\n",
    "os.makedirs(export_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through each cleaned DataFrame and save it using its original filename\n",
    "for i, df in enumerate(dataframes):\n",
    "    # Use the corresponding original filename from `csv_files`\n",
    "    filename = csv_files[i]  # Fetch the original filename\n",
    "    file_path = os.path.join(export_directory, filename)\n",
    "    \n",
    "    # Export the cleaned DataFrame to CSV\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to GBQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpq_path = \"C:/Documents/Business Analytics Grad/Applied Data Analytics/wedge/\"\n",
    "gbq_project_id = \"rhamre\"\n",
    "gbq_dataset_id = \"wedge\"\n",
    "\n",
    "private_key = \"C:/Documents/Business Analytics Grad/Applied Data Analytics/wedge/data/rhamre-10ae9aad8c6b.json\"\n",
    "\n",
    "gbq_credentials = service_account.Credentials.from_service_account_file(private_key)\n",
    "client = bigquery.Client(credentials=gbq_credentials, project=gbq_project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "job_config.schema_update_options = [bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config.schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memType\", \"STRING\", mode=\"NULLABLE\"),  # Changed from BOOLEAN to STRING\n",
    "    bigquery.SchemaField(\"staff\", \"STRING\", mode=\"NULLABLE\"),    # Changed from BOOLEAN to STRING\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"STRING\", mode=\"NULLABLE\"),  # Changed from BOOLEAN to STRING\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"display\", \"STRING\", mode=\"NULLABLE\"),  # Changed from BOOLEAN to STRING\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"match_id\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_id\", \"FLOAT\", mode=\"NULLABLE\")\n",
    "]\n",
    "\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over files and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data/full_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_exists(client, table_ref):\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_files = os.listdir(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clean_file_name in clean_files:\n",
    "    tab, other = clean_file_name.split(\".csv\")      #change if file names are different, for example \"_clean\" or \".csv\"\n",
    "    table_full_name = \".\".join([gbq_project_id, gbq_dataset_id, tab])\n",
    "\n",
    "    if not table_exists(client, table_full_name):\n",
    "        table_ref = client.create_table(table = table_full_name)\n",
    "    else:\n",
    "        table_ref = client.get_table(table_full_name)\n",
    "\n",
    "    with open(data_directory + clean_file_name, mode = \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            table_ref,\n",
    "            location = \"US\",\n",
    "            job_config = job_config\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=rhamre, location=US, id=217c9336-bea4-416c-9735-118e56c19c63>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i, df in enumerate(dataframes):\\n    print(f\"DataFrame {i + 1}:\")\\n    print(f\"  Shape: {df.shape}\")\\n    print(f\"  Columns: {df.columns.tolist()}\")\\n    print(\"-\" * 40)\\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at dataframes\n",
    "'''\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"DataFrame {i + 1}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {df.columns.tolist()}\")\n",
    "    print(\"-\" * 40)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndataset_ref = bigquery.DatasetReference(gbq_project_id, gbq_dataset_id)\\n\\n# List all tables in the dataset\\ntables = client.list_tables(dataset_ref)\\n\\n# Loop through each table and delete it\\nfor table in tables:\\n    table_id = f\"{table.project}.{table.dataset_id}.{table.table_id}\"\\n    print(f\"Deleting table: {table_id}\")\\n    client.delete_table(table_id)  # Make an API request to delete the table\\n    print(f\"Deleted table {table_id}\")\\n\\n\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete all tables under wegde\n",
    "'''\n",
    "dataset_ref = bigquery.DatasetReference(gbq_project_id, gbq_dataset_id)\n",
    "\n",
    "# List all tables in the dataset\n",
    "tables = client.list_tables(dataset_ref)\n",
    "\n",
    "# Loop through each table and delete it\n",
    "for table in tables:\n",
    "    table_id = f\"{table.project}.{table.dataset_id}.{table.table_id}\"\n",
    "    print(f\"Deleting table: {table_id}\")\n",
    "    client.delete_table(table_id)  # Make an API request to delete the table\n",
    "    print(f\"Deleted table {table_id}\")\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
